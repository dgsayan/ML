{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The k-NN algorithm\n",
    "\n",
    "Task: Identify the Nearest Neighbors\n",
    "\n",
    "### 1. Concept\n",
    "\n",
    "1.1. <ins>Assumption</ins>: Similar inputs have similar outputs\n",
    "\n",
    "1.2. <ins>Summary</ins>: For a test input $x$, the most common label among its $k$ most similar training samples, is returned as its output label\n",
    "\n",
    "1.3. <ins>Formal definition</ins>:\n",
    "\n",
    "> Test point : $x$<br>\n",
    "> Training Data : $D$\n",
    "\n",
    "> Denote the set of $k$ nearest neighbors of $x$ as $S_x$, such that $S_x \\in D$ and $|S_x| = k$<br>\n",
    "> Then ∀ $(x',y') \\in D\\setminus{}S_x$ and ∀ $(x'',y'') \\in S_x$\n",
    "> $$ dist(x, x') >= max (dist(x, x'')) $$\n",
    "> That is, every point in $D$ but not in $S_x$, is at least as far away from $x$ as the farthest point in $S_x$ <br>\n",
    "\n",
    "> We can then define the classifier $h()$ as a function returning the most common label in $S_x$\n",
    "> $$ h(x) = mode({y'' : (x'',y'') \\in S_x}) $$\n",
    "\n",
    "\n",
    "1.4. <ins>Distance Metric</ins>\n",
    "\n",
    "The k-NN classifier fundamentally relies on a distance metric to identify the nearest neighbors. The better that metric quantifies similarity, the better the classification.The most common choice is the **Minkowski Distance**\n",
    "\n",
    "$$ dist(x, z) = (\\displaystyle\\sum_{i=1}^{d}|x_i-z_i|^{p}) ^{1/p} $$\n",
    "\n",
    "> p = 1 gives us Manhattan distance<br>\n",
    "> p = 2 gives us Eucledian distance<br>\n",
    "> ...\n",
    "\n",
    "kNN is a non-parametric learner i.e. it does not make assumptions about the form of the mapping function. The entire training data is stored in the model, which becomes slower thus with more data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pseudocode\n",
    "\n",
    "$\\text{Classify}(\\mathbf{X,Y},x)\\;\\text{//}\\;\\mathbf{X}: m\\;\\text{rows of training data},\\;\\mathbf{Y}:\\text{class labels of}\\;\\mathbf{X},\\;x:\\text{test point}$\n",
    "\n",
    "$\\mathbf{for}\\;i = 1\\;\\text{to}\\;m\\;\\mathbf{do}$<br>\n",
    "$\\hspace{3ex}\\text{Compute distance}\\;d(\\mathbf{X}_{i},x)$<br>\n",
    "$\\mathbf{end\\;for}$\n",
    "\n",
    "$\\text{Compute set}\\;I\\;\\text{containing indices for the}\\;k\\;\\text{smallest distances in}\\;d(\\mathbf{X}_{i},x)$<br>\n",
    "$\\mathbf{return}\\;\\text{majority class label for}\\;\\{\\mathbf{Y_i},\\;\\text{where}\\;i\\in I\\}$\n",
    "\n",
    "Execution time for a test point O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementation\n",
    "\n",
    "Data: The Iris data set contains 3 types of Iris flower with 50 instances each.<br>\n",
    "Predicted attribute : class of Iris plant<br>\n",
    "Information attributes : sepal length, sepal width, petal length, petal width\n",
    "\n",
    "In the following section, we build a KNN classifier in python and make predictions for some test points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "X = np.genfromtxt('datasets\\iris.csv', delimiter = ',', dtype = None, skip_header = 1, encoding = 'UTF-8', usecols = [0,1,2,3])\n",
    "Y = np.genfromtxt('datasets\\iris.csv', delimiter = ',', dtype = None, skip_header = 1, encoding = 'UTF-8', usecols = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train-test\n",
    "\n",
    "np.random.seed(0)\n",
    "ids = np.arange(Y.shape[0])\n",
    "test_ids = np.random.choice(ids, size = 5, replace = False)\n",
    "train_ids = ids[~np.isin(ids, test_ids)]\n",
    "\n",
    "X_train = X[train_ids]\n",
    "Y_train = Y[train_ids]\n",
    "x_test = X[test_ids]\n",
    "y_test = Y[test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minkowski distance\n",
    "\n",
    "def minkowski_dist(v1, v2, p):\n",
    "    return np.power(np.sum(np.power(abs(v1-v2), p)), 1/p)\n",
    "\n",
    "# Mode 1-d\n",
    "\n",
    "def mode1d(v):\n",
    "    vals, cnt = np.unique(v, return_counts = True)\n",
    "    return vals[cnt.argmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN\n",
    "\n",
    "def kNN_classify(X, Y, x, p, k):\n",
    "    \n",
    "    dist_arr = np.fromiter((minkowski_dist(v, x, p) for v in X), 'float')\n",
    "    \n",
    "    idx_k_smallest = np.argpartition(dist_arr, k)[:k]\n",
    "    \n",
    "    return mode1d(Y[idx_k_smallest])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction\n",
    "\n",
    "pred = np.fromiter((kNN_classify(X_train, Y_train, x, 2, 3) for x in x_test), '<U12')\n",
    "\n",
    "flag = pred == y_test\n",
    "\n",
    "accuracy = sum(flag)/len(flag)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1. <ins> Convergence of 1-NN</ins>\n",
    "\n",
    "Absract: As $n \\rightarrow \\infty$, the 1NN error is no more than twice the error of the Bayes Optimal Classifier.\n",
    "\n",
    "$x_t$ : test point <br>\n",
    "$x_{NN}$ : nearest neighbor of $x_t$\n",
    "\n",
    "As $n \\rightarrow \\infty, \\hspace{2ex} dist(x_t, x_{NN}) \\rightarrow 0 \\; i.e. x_{NN} = x_t$<br>\n",
    "\n",
    "The bayes optimal classifier provides a theoretical lower bound of error for a given feature respresntation.\n",
    "\n",
    "Probable error rate of the bayes optimal classifier $\\epsilon_{BayesOptCl} \\; = \\; 1 - P(y=1|x_t)$ \n",
    "\n",
    "Probable error rate of 1-NN classifier: $\\epsilon_{1NN} \\; = \\; P(y=1|x_{NN}) \\times P(y=0|x_t) + P(y=0|x_{NN}) \\times P(y=1|x_t)$\n",
    "\n",
    "or, $\\epsilon_{1NN} \\; = \\; P(y=1|x_{NN}) \\times(1 - P(y=1|x_t)) + (1 - P(y=1|x_{NN})) \\times P(y=1|x_t)$\n",
    "\n",
    "or, $\\epsilon_{1NN} \\; <= \\; (1 - P(y=1|x_t)) + (1 - P(y=1|x_{NN}))$\n",
    "\n",
    "As $n \\rightarrow \\infty$, $ x_{NN} = x_t\\;$ or, $\\epsilon_{1NN} \\; <= \\; 2(1 - P(y=1|x_t))$\n",
    "\n",
    "$\\therefore \\epsilon_{1NN} <= 2\\times\\epsilon_{BayesOptCl}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2. <ins>Curse of Dimensionality</ins>\n",
    "\n",
    "Abstract: In high dimensional spaces, the kNN assumption breaks down beacuse points drawn from a probability distribution never tend to be close together.\n",
    "\n",
    "Let us consider a unit hypercube $[0,1]^d$ which encloses $n$ uniformly sampled training points as 'O'\n",
    "\n",
    "Let $l$ be the edge length of the smallest hyper-cube 'I' that contains all k-nearest neighbors of a test point.\n",
    "\n",
    "$\\therefore \\frac{\\text{Volume of inner cube I}}{\\text{Volume of outer cube O}} \\simeq \\frac{\\text{training samples within I}}{\\text{training samples within O}}$\n",
    "\n",
    "or, $\\dfrac{l^d}{1^d} \\simeq \\dfrac{k}{n}$\n",
    "\n",
    "or, $l \\simeq (k/n)^{1/d}$\n",
    "\n",
    "if $n$ = 1000 and $k$ = 10, then $l$ varies with $d$ as:\n",
    "\n",
    "| d | l |\n",
    "|:-:|:-:|\n",
    "| 2 | 0.1|\n",
    "|10 | 0.631|\n",
    "|100 | 0.955|\n",
    "|1000| 0.995|\n",
    "\n",
    "As $d>>0$, almost the entire space within the unit hypercube is needed to fit the inner hypercube of 10-NN. This implies that the $k$-NN are not particularly closer (and therefore more similar) than any other data points in the training set in a high dimensional space, and thus, the similarity assumption of kNN breaks down.\n",
    "\n",
    "Although K-NN is hence best suited for low dimensional data, it may still be effective high dimensional spaces if the data lie on a low dimensional subspace or manifold within that higher dimensional space e.g. natural images (digits, faces)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3. <ins> kNN for Regression </ins>\n",
    "\n",
    "The kNN alogorithm can be used for regression (where $y_i \\in \\mathbb{R}$). The mean of actual values of $k$ nearest neighbors of test point $x_t$ is returned as the predicted value for $x_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4. <ins> Metric Learning </ins>\n",
    "\n",
    "The distance metric quantifies similarity between points and helps identify the 'neighbors'. While the Minkowski distance is the most widely used distance metric, kNN becomes truly competitive (higher accuracy) through Metric Learning, where the Mahalanobis distance metric is *learned* from the labeled samples and used to compute distance for kNN classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Closing Notes\n",
    "\n",
    "k-NN is effective if distance reliably reflect a semantically meaningful notion of similarity. \n",
    "\n",
    "As $n \\rightarrow \\infty$ it is provably very accurate yet very slow. \n",
    "\n",
    "As $d>>0$, points drawn from a probability distribution stop being similar to each other and the kNN assumption breaks down"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
